# model architecture
batch_size: 20
timesteps: 35
num_dims: 1100
n_layers: 2
word_dims: 650
char_dims: 15
num_units: 650
lstm_clip: 10.0
grad_clip: 5.0
keep_prob: 1.0 # disable dropout for now
kernel_sizes:
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  - 7
kernel_features:
  - 50
  - 100
  - 150
  - 200
  - 200
  - 200
  - 200

# vocabulary details
word_vocab_size: -1 # to be initialized at run-time
char_vocab_size: -1 # to be initialized at run-time
max_word_length: 65 # after including <w> and </w>
include_word_markers: True # whether to include <w> and </w> or not

# training parameters
epochs: 30
initial_learning_rate: 1.0
lr_decay: 0.5

# fine-tuning constants
fine_tune_pos_words: 3
fine_tune_neg_words: 3
fine_tune_delta: 0.6
fine_tune_reg: 1e-9
fine_tune_grad_clip: 2
fine_tune_lr: 0.05
fine_tune_num_iters: 100
fine_tune_cue_threshold: 5

# testing parameters
max_tokens: 30